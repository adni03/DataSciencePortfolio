[{"categories":["Natural Language Processing"],"content":"Classifying news articles as real or fake using Natural Language Processing techniques. With the advent of the internet, the world has never been so informed. News can be shared across the globe within seconds through online sources such at news websites, blog posts, twitter etc. But, the grim reality is that there is a lot of misinformation and disinformation on the internet. Most recently, during the peak of the COVID-19 pandemic, misinformatino about vaccines spread like wildfire. Why don’t we use NLP to combat this? ","date":"2022-09-04","objectID":"/DataSciencePortfolio/fake-news-classification/:0:0","tags":["LSTMs","TensorFlow","nltk"],"title":"Fake News Classification using LSTMs","uri":"/DataSciencePortfolio/fake-news-classification/"},{"categories":["Natural Language Processing"],"content":"1. The Dataset The dataset consists of news article gathers from some of the major sources in the United States such as The Washington Post, Reuters, New York Times, etc. These articles cover the following topics: Politics World News Government News Middle-east News The structure of the dataset is: title: Title of the news article text: The content of the article (whole) subject: One of the above mentioned topics date: Date of publish isfake: A binary varible where 1 is Fake and 0 is Real ","date":"2022-09-04","objectID":"/DataSciencePortfolio/fake-news-classification/:1:0","tags":["LSTMs","TensorFlow","nltk"],"title":"Fake News Classification using LSTMs","uri":"/DataSciencePortfolio/fake-news-classification/"},{"categories":["Natural Language Processing"],"content":"2. Data Cleaning Each news article from the corpus was stripped down by removing the stopwords. The list of stopwords were obtained from the nltk and the gensim libraries. # download stopwords nltk.download(\"stopwords\") # Obtain additional stopwords from nltk from nltk.corpus import stopwords stop_words = stopwords.words('english') stop_words.extend(['from', 'subject', 're', 'edu', 'use']) gensim.parsing.preprocessing.STOPWORDS The news article was tokenized and each token was checked against the list of stopwords. The remaining tokens were stored in a list and then joined to form a clean article. # Remove stopwords and remove words with 2 or less characters def preprocess(text): result = [] for token in gensim.utils.simple_preprocess(text): if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) \u003e 3 and token not in stop_words: result.append(token) return result df['clean'] = df['original'].apply(preprocess) df['clean_joined'] = df['clean'].apply(lambda x: \" \".join(x)) ","date":"2022-09-04","objectID":"/DataSciencePortfolio/fake-news-classification/:2:0","tags":["LSTMs","TensorFlow","nltk"],"title":"Fake News Classification using LSTMs","uri":"/DataSciencePortfolio/fake-news-classification/"},{"categories":["Natural Language Processing"],"content":"3. Exploratory Data Analysis To get a better understanding of the dataset, WordCloud was used to see the most frequently used terms in the fake and real news articles. ","date":"2022-09-04","objectID":"/DataSciencePortfolio/fake-news-classification/:3:0","tags":["LSTMs","TensorFlow","nltk"],"title":"Fake News Classification using LSTMs","uri":"/DataSciencePortfolio/fake-news-classification/"},{"categories":["Natural Language Processing"],"content":"Word Cloud for fake news # plot the word cloud for text that is fake plt.figure(figsize = (20,20)) wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.isfake == 1].clean_joined)) plt.imshow(wc, interpolation = 'bilinear') Fake News ","date":"2022-09-04","objectID":"/DataSciencePortfolio/fake-news-classification/:3:1","tags":["LSTMs","TensorFlow","nltk"],"title":"Fake News Classification using LSTMs","uri":"/DataSciencePortfolio/fake-news-classification/"},{"categories":["Natural Language Processing"],"content":"Word Cloud for real news # plot the word cloud for text that is fake plt.figure(figsize = (20,20)) wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(df[df.isfake == 0].clean_joined)) plt.imshow(wc, interpolation = 'bilinear') Real News ","date":"2022-09-04","objectID":"/DataSciencePortfolio/fake-news-classification/:3:2","tags":["LSTMs","TensorFlow","nltk"],"title":"Fake News Classification using LSTMs","uri":"/DataSciencePortfolio/fake-news-classification/"},{"categories":["Natural Language Processing"],"content":"4. Preparing data by tokenization and padding Once the data is cleaned, I created train-test splits using train_test_split from the sklearn library. Then a Tokenizer from the nltk library was used to generate sequences of tokenized words. tokenizer = Tokenizer(num_words = total_words) tokenizer.fit_on_texts(x_train) train_sequences = tokenizer.texts_to_sequences(x_train) test_sequences = tokenizer.texts_to_sequences(x_test) Next, each sequence had to be padded to the maximum length of article in the news corpus. padded_train = pad_sequences(train_sequences,maxlen = 4405, padding = 'post', truncating = 'post') padded_test = pad_sequences(test_sequences,maxlen = 4405, truncating = 'post') ","date":"2022-09-04","objectID":"/DataSciencePortfolio/fake-news-classification/:4:0","tags":["LSTMs","TensorFlow","nltk"],"title":"Fake News Classification using LSTMs","uri":"/DataSciencePortfolio/fake-news-classification/"},{"categories":["Natural Language Processing"],"content":"4. Building the Model Now for the fun part! The model I used for this task was a Bidirectional LSTM with 128 units. I had to use a bidirectional model because the length of these articles are quite long and so I wanted to capture long term dependencies from the future and the past. To train the model I used the TensorFlow framework. An Embedding layer was added before the BiLSTM layer to convert the sequences into Embeddings for the model to train on. Since this is a binary classification problem, the output layer has one unit with a sigmoid activation. The adam optimizer was used and the loss function was set to binary_crossentropy. ","date":"2022-09-04","objectID":"/DataSciencePortfolio/fake-news-classification/:5:0","tags":["LSTMs","TensorFlow","nltk"],"title":"Fake News Classification using LSTMs","uri":"/DataSciencePortfolio/fake-news-classification/"},{"categories":["Natural Language Processing"],"content":"Code # Sequential Model model = Sequential() # embeddidng layer model.add(Embedding(total_words, output_dim = 128)) # model.add(Embedding(total_words, output_dim = 240)) # Bi-Directional RNN and LSTM model.add(Bidirectional(LSTM(128))) # Dense layers model.add(Dense(128, activation = 'relu')) model.add(Dense(1,activation= 'sigmoid')) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc']) ","date":"2022-09-04","objectID":"/DataSciencePortfolio/fake-news-classification/:5:1","tags":["LSTMs","TensorFlow","nltk"],"title":"Fake News Classification using LSTMs","uri":"/DataSciencePortfolio/fake-news-classification/"},{"categories":["Natural Language Processing"],"content":"5. Training and Performance The model was trained for 2 epochs with a validation split of 0.1 and a batch_size of 64. The trained model was then evaluated on the test set by obtaining the sigmoid outputs. If the value is greater than 0.5, classify as fake. ","date":"2022-09-04","objectID":"/DataSciencePortfolio/fake-news-classification/:6:0","tags":["LSTMs","TensorFlow","nltk"],"title":"Fake News Classification using LSTMs","uri":"/DataSciencePortfolio/fake-news-classification/"},{"categories":["Natural Language Processing"],"content":"Code # train the model model.fit(padded_train, y_train, batch_size = 64, validation_split = 0.1, epochs = 2) # if the predicted value is \u003e0.5 it is real else it is fake prediction = [] for i in range(len(pred)): if pred[i].item() \u003e 0.5: prediction.append(1) else: prediction.append(0) # getting the accuracy from sklearn.metrics import accuracy_score accuracy = accuracy_score(list(y_test), prediction) print(\"Model Accuracy : \", accuracy) Model Accuracy : 0.9968819599109131 ","date":"2022-09-04","objectID":"/DataSciencePortfolio/fake-news-classification/:6:1","tags":["LSTMs","TensorFlow","nltk"],"title":"Fake News Classification using LSTMs","uri":"/DataSciencePortfolio/fake-news-classification/"},{"categories":["Natural Language Processing"],"content":"6. Results With a test accuracy of 99.6%, this model does a great job at detecting if a news article is real or fake. ","date":"2022-09-04","objectID":"/DataSciencePortfolio/fake-news-classification/:7:0","tags":["LSTMs","TensorFlow","nltk"],"title":"Fake News Classification using LSTMs","uri":"/DataSciencePortfolio/fake-news-classification/"},{"categories":["Computer Vision"],"content":"Merging a content image and a style image to produce art! Neural Style Transfer (NST) is an interesting optimization technique in deep learning. It merges to images: a “content” image (C) and a “style” image (S), to create a “generated” image (G). NST uses a previously trained convolutional network, and builds on top of that. This is called transfer learning. In this project, I will be using the VGG network from the original NST paper. Specifically, I will be using the VGG-19 network, a 19-layer version of the network. This model has been trained on the ImageNet dataset. ","date":"2022-08-12","objectID":"/DataSciencePortfolio/neural-style-transfer/:0:0","tags":["VGG-19","TensorFlow"],"title":"Neural Style Transfer","uri":"/DataSciencePortfolio/neural-style-transfer/"},{"categories":["Computer Vision"],"content":"1. Style Cost Function Firstly, to compute the “style” cost, we need to calculate the Gram matrix. This matrix gives us an idea of how similar two feature maps are. The code to compute the Gram matrix is given below. It is just the dot product of the input vectors. We are interested in similarity since we want the style of the Style image and the Generated image to the similar. Next, once we have the Gram matrices calculated for the Style image and the Generated image, a squared difference can be taken with a normalization constant to obtain the Style loss for a particular layer. ","date":"2022-08-12","objectID":"/DataSciencePortfolio/neural-style-transfer/:1:0","tags":["VGG-19","TensorFlow"],"title":"Neural Style Transfer","uri":"/DataSciencePortfolio/neural-style-transfer/"},{"categories":["Computer Vision"],"content":"Code def gram_matrix(A): GA = tf.matmul(A, tf.transpose(A)) return GA def compute_layer_style_cost(a_S, a_G): _, n_H, n_W, n_C = a_G.get_shape() a_S = tf.transpose(tf.reshape(a_S, shape=[-1, n_C])) a_G = tf.transpose(tf.reshape(a_G, shape=[-1, n_C])) GS = gram_matrix(a_S) GG = gram_matrix(a_G) J_style_layer = tf.reduce_sum(tf.square(tf.subtract(GS, GG)), axis=None)/((2 * n_W * n_H * n_C) ** 2) return J_style_layer ","date":"2022-08-12","objectID":"/DataSciencePortfolio/neural-style-transfer/:1:1","tags":["VGG-19","TensorFlow"],"title":"Neural Style Transfer","uri":"/DataSciencePortfolio/neural-style-transfer/"},{"categories":["Computer Vision"],"content":"2. Content Cost Function In the shallower layers of a CNN, the model learns low level features whereas in the deeper layers, the model learns complex features. Hence, in the generated image, the content should match that of the input content image. This can be achieved by taking activations from the middle of the CNN. This cost can be computed by comparing the activations of the style image and the generated image. ","date":"2022-08-12","objectID":"/DataSciencePortfolio/neural-style-transfer/:2:0","tags":["VGG-19","TensorFlow"],"title":"Neural Style Transfer","uri":"/DataSciencePortfolio/neural-style-transfer/"},{"categories":["Computer Vision"],"content":"Code def compute_content_cost(content_output, generated_output): a_C = content_output[-1] a_G = generated_output[-1] m, n_H, n_W, n_C = a_G.get_shape() a_C_unrolled = tf.reshape(a_C, shape=[m, -1, n_C]) a_G_unrolled = tf.reshape(a_G, shape=[m, -1, n_C]) J_content = tf.reduce_sum(tf.square(tf.subtract(a_G_unrolled, a_G_unrolled)), axis=None)/(4 * n_W * n_C * n_H) return J_content ","date":"2022-08-12","objectID":"/DataSciencePortfolio/neural-style-transfer/:2:1","tags":["VGG-19","TensorFlow"],"title":"Neural Style Transfer","uri":"/DataSciencePortfolio/neural-style-transfer/"},{"categories":["Computer Vision"],"content":"3. Total Cost The total cost function is just a weighted sum of the Style Cost and the Content Cost. ","date":"2022-08-12","objectID":"/DataSciencePortfolio/neural-style-transfer/:3:0","tags":["VGG-19","TensorFlow"],"title":"Neural Style Transfer","uri":"/DataSciencePortfolio/neural-style-transfer/"},{"categories":["Computer Vision"],"content":"Code @tf.function() def total_cost(J_content, J_style, alpha = 10, beta = 40): return alpha * J_content + beta * J_style ","date":"2022-08-12","objectID":"/DataSciencePortfolio/neural-style-transfer/:3:1","tags":["VGG-19","TensorFlow"],"title":"Neural Style Transfer","uri":"/DataSciencePortfolio/neural-style-transfer/"},{"categories":["Computer Vision"],"content":"4. Optimization Loop The various steps for solving the optimization problem are: Load the content image Load the style image Randomly initialize the image to be generated Load the VGG19 model Compute the content cost Compute the style cost Compute the total cost Define the optimizer and learning rate The training loop looks like: ","date":"2022-08-12","objectID":"/DataSciencePortfolio/neural-style-transfer/:4:0","tags":["VGG-19","TensorFlow"],"title":"Neural Style Transfer","uri":"/DataSciencePortfolio/neural-style-transfer/"},{"categories":["Computer Vision"],"content":"Code optimizer = tf.keras.optimizers.Adam(learning_rate=0.03) @tf.function() def train_step(generated_image): with tf.GradientTape() as tape: a_G = vgg_model_outputs(generated_image) J_style = compute_style_cost(a_S, a_G) J_content = compute_content_cost(a_C, a_G) J = total_cost(J_content, J_style, alpha=20, beta=40) grad = tape.gradient(J, generated_image) optimizer.apply_gradients([(grad, generated_image)]) generated_image.assign(clip_0_1(generated_image)) return J ","date":"2022-08-12","objectID":"/DataSciencePortfolio/neural-style-transfer/:4:1","tags":["VGG-19","TensorFlow"],"title":"Neural Style Transfer","uri":"/DataSciencePortfolio/neural-style-transfer/"},{"categories":["Computer Vision"],"content":"Detecting the presence of COVID-19 in a patient’s lungs by analyzing chest X-Rays using CNNs. COVID-19 is an infection that is caused by the SARS-Cov2 virus. There are multiple variants of this virus, as the world has come to know it, but one particularly dangerous one is the Delta variant, which is known to cause damage to the lungs of its host. This infection can be spotted via a chest X-RAY scan. This scan is the basis for this project. Using a Convolutional Neural Network, these scans are analyzed for signs of COVID-19 infection. The tricky bit is differentiating between scans of patients with viral pneumonia and COVID-19. The latter stages of the COVID infection is brought on by a pneumonia like infection in the lungs. This project was implemented in the PyTorch framework. ","date":"2022-07-04","objectID":"/DataSciencePortfolio/covid19-detection-cnns/:0:0","tags":["ResNets","COVID-19","PyTorch"],"title":"COVID-19 Detection using CNNs","uri":"/DataSciencePortfolio/covid19-detection-cnns/"},{"categories":["Computer Vision"],"content":"1. The Dataset The COVID-19 Radiograpy Database is a collection of chest X-RAYS of patients with the following conditions: COVID-19 infection: 3616 images Normal lungs: 10,192 images Non-COVID lung infections: 6012 images Viral pneumonia infection: 1345 images This database was created by researchers from Qatar Univeristy and Univeristy of Dhaka in collaboration with medical professionals in their countries as well as Malaysia and Pakistan. ","date":"2022-07-04","objectID":"/DataSciencePortfolio/covid19-detection-cnns/:1:0","tags":["ResNets","COVID-19","PyTorch"],"title":"COVID-19 Detection using CNNs","uri":"/DataSciencePortfolio/covid19-detection-cnns/"},{"categories":["Computer Vision"],"content":"2. Custom Dataset I created a custom dataset class that is helpful while training and testing the model. This class inherits from torch.utils.data.Dataset and implements the __getitem()__ method. ","date":"2022-07-04","objectID":"/DataSciencePortfolio/covid19-detection-cnns/:2:0","tags":["ResNets","COVID-19","PyTorch"],"title":"COVID-19 Detection using CNNs","uri":"/DataSciencePortfolio/covid19-detection-cnns/"},{"categories":["Computer Vision"],"content":"Code class ChestXRayDataset(torch.utils.data.Dataset): def __init__(self, image_dirs, transform): # transform obj is used to do data augmentation def get_images(class_name): images = [x for x in os.listdir(image_dirs[class_name]) if x[-3:].lower().endswith('png')] print(f'Found {len(images)} {class_name} examples') return images self.images = {} self.class_names = ['normal', 'viral', 'covid'] for c in self.class_names: self.images[c] = get_images(c) self.image_dirs = image_dirs self.transform = transform def __len__(self): return sum([len(self.images[c]) for c in self.class_names]) def __getitem__(self, index): class_name = random.choice(self.class_names) index = index % len(self.images[class_name]) image_name = self.images[class_name][index] image_path = os.path.join(self.image_dirs[class_name], image_name) image = Image.open(image_path).convert('RGB') return self.transform(image), self.class_names.index(class_name) ","date":"2022-07-04","objectID":"/DataSciencePortfolio/covid19-detection-cnns/:2:1","tags":["ResNets","COVID-19","PyTorch"],"title":"COVID-19 Detection using CNNs","uri":"/DataSciencePortfolio/covid19-detection-cnns/"},{"categories":["Computer Vision"],"content":"3. Image Tranforms A few preprocessing steps were added to the pipeline before the model could be trained. For the training images: Resizing: match input dimensions for pretrained ResNet18 Augmentation: RandomHorizontalFlip Normalization And for the test images: Resizing Normalization Normalization was done separately to avoid Data Leakage. ","date":"2022-07-04","objectID":"/DataSciencePortfolio/covid19-detection-cnns/:3:0","tags":["ResNets","COVID-19","PyTorch"],"title":"COVID-19 Detection using CNNs","uri":"/DataSciencePortfolio/covid19-detection-cnns/"},{"categories":["Computer Vision"],"content":"4. The Model The Convolutional Neural Network used for this project is a relatively light-weight residual network, ResNet18. This model was chosen for ease of transfer learning. The model was pretrained on the ImageNet dataset, which consists of images from over 1000 classes. ResNet Model ","date":"2022-07-04","objectID":"/DataSciencePortfolio/covid19-detection-cnns/:4:0","tags":["ResNets","COVID-19","PyTorch"],"title":"COVID-19 Detection using CNNs","uri":"/DataSciencePortfolio/covid19-detection-cnns/"},{"categories":["Computer Vision"],"content":"Code resnet18 = torchvision.models.resnet18(pretrained=True) ","date":"2022-07-04","objectID":"/DataSciencePortfolio/covid19-detection-cnns/:4:1","tags":["ResNets","COVID-19","PyTorch"],"title":"COVID-19 Detection using CNNs","uri":"/DataSciencePortfolio/covid19-detection-cnns/"},{"categories":["Computer Vision"],"content":"5. Training and Performance The model was trained for a few epochs to fine-tune the weights to improve the prediction accuracy. The performance criteria was set to 95% accuracy and the model was able to do so in under 2 epochs. The training loop is highlighted below, for full code, please visit my GitHub repository for this project. ","date":"2022-07-04","objectID":"/DataSciencePortfolio/covid19-detection-cnns/:5:0","tags":["ResNets","COVID-19","PyTorch"],"title":"COVID-19 Detection using CNNs","uri":"/DataSciencePortfolio/covid19-detection-cnns/"},{"categories":["Computer Vision"],"content":"Code def train(epochs): for e in range(epochs): train_loss = 0 resnet18.train() # batch of images for train_step, (images, labels) in enumerate(dl_train): optimizer.zero_grad() outputs = resnet18(images) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() train_loss += loss.item() if train_step % 20 == 0: # perform validation calculations resnet18.train() if acc \u003e 0.95: print('Performance condition satisfied') return train_loss /= (train_step + 1) print(f'Training loss: {train_loss:.4f}') ","date":"2022-07-04","objectID":"/DataSciencePortfolio/covid19-detection-cnns/:5:1","tags":["ResNets","COVID-19","PyTorch"],"title":"COVID-19 Detection using CNNs","uri":"/DataSciencePortfolio/covid19-detection-cnns/"},{"categories":["Computer Vision"],"content":"6. Results With an accuracy of 95%, this model can be used as a tool to detect COVID-19 in the lungs of patients. It cannot, in any capacity, be a subsitute for a medical professional. ","date":"2022-07-04","objectID":"/DataSciencePortfolio/covid19-detection-cnns/:6:0","tags":["ResNets","COVID-19","PyTorch"],"title":"COVID-19 Detection using CNNs","uri":"/DataSciencePortfolio/covid19-detection-cnns/"},{"categories":["Computer Vision"],"content":"Classifying images of human faces into one of seven basic emotions. The goal of this project was to classify images of human faces into one of seven basic emotions. To do so, I used Convolutional Neural Networks as the advantage it had over its precedessors is that it can detect important features without human intervention. CNNs are computationally efficient as well due to the special convolution and pooling operations. ","date":"2021-02-04","objectID":"/DataSciencePortfolio/facial-emotion-recognition/:0:0","tags":["ResNets","TensorFlow","OpenCV"],"title":"Facial Emotion Recognition","uri":"/DataSciencePortfolio/facial-emotion-recognition/"},{"categories":["Computer Vision"],"content":"1. The Dataset The FER-2013 dataset consists of 35,887 images, each of size 48x48 pixels. The data was split into 28,709 images for training and 7,178 images for testing. ","date":"2021-02-04","objectID":"/DataSciencePortfolio/facial-emotion-recognition/:1:0","tags":["ResNets","TensorFlow","OpenCV"],"title":"Facial Emotion Recognition","uri":"/DataSciencePortfolio/facial-emotion-recognition/"},{"categories":["Computer Vision"],"content":"2. The Model A ResNet50 architecture was used as the Convolutional Neural Network which was pretrained on the ImageNet dataset. This dataset has 1000 classes but for the case of facial emotion recognition, there are only 7. Hence, to fine tune the network, the shallower layers were frozen and the model was trained using the training set. The building of the fine-tuned network is shown below: ","date":"2021-02-04","objectID":"/DataSciencePortfolio/facial-emotion-recognition/:2:0","tags":["ResNets","TensorFlow","OpenCV"],"title":"Facial Emotion Recognition","uri":"/DataSciencePortfolio/facial-emotion-recognition/"},{"categories":["Computer Vision"],"content":"Code def fer_model_fine_tune(base_model, input_shape): fine_tune_start = 155 # Freezing all layers before the fine_tune_start layer for layer in base_model.layers[: fine_tune_start]: layer.trainable = None # declaring the input inputs = Input(shape=input_shape) # obtaining image embeddings from the second last layer x = base_model(inputs, training=False) # adding global avg pooling x = GlobalAvgPool2D()(x) # including dropout with prob=0.2 x = Dropout(rate=0.2)(x) # adding a flatten layer x = Flatten()(x) # adding the prediction layer with 7 units, softmax activation prediction_layer = Dense(units=7, activation='softmax')(x) # outputs outputs = prediction_layer model = Model(inputs, outputs) return model ","date":"2021-02-04","objectID":"/DataSciencePortfolio/facial-emotion-recognition/:2:1","tags":["ResNets","TensorFlow","OpenCV"],"title":"Facial Emotion Recognition","uri":"/DataSciencePortfolio/facial-emotion-recognition/"},{"categories":["Computer Vision"],"content":"3. Training and Performance The final layer of the base pretrained ResNet50 was replaced with a 7 unit softmax activated layer. This model was trained for 20 epochs and it obtained an accuracy of 85.2%. To improve the performance on the FER dataset, freezing of some of the layers was done and retrained for 20 epochs. The accuracy increased from 85.2% to 86.3%. loss, accuracy, precision, recall, auc = fer_model_2.evaluate(test_dataset) print('Accuracy: {}'.format(accuracy)) 113/113 [==============================] - 20s 176ms/step - loss: 1.6389 - accuracy: 0.8633 - precision: 0.6699 - recall: 0.0854 - auc: 0.7451 Accuracy: 0.8633323907852173 ","date":"2021-02-04","objectID":"/DataSciencePortfolio/facial-emotion-recognition/:3:0","tags":["ResNets","TensorFlow","OpenCV"],"title":"Facial Emotion Recognition","uri":"/DataSciencePortfolio/facial-emotion-recognition/"},{"categories":["Machine Learning"],"content":"Predicting the probability with which customers default on their bills.","date":"2019-10-01","objectID":"/DataSciencePortfolio/credit-card-risk-analyzer/","tags":["Sci-kit Learn","Logistic Regression","GridSearchCV"],"title":"Credit Card Risk Analyzer","uri":"/DataSciencePortfolio/credit-card-risk-analyzer/"},{"categories":["Machine Learning"],"content":"Predicting the probability with which customers default on their bills. In this project, I leveraged the power of Logistic Regression to predict if a customer’s credit card application will be approved. Most banks have systems like this, albeit much more complex, to make a prediction on the application’s approval. The risk associated with lending credit is immense and hence, banks rely heavily on such systems to mitigate it. ","date":"2019-10-01","objectID":"/DataSciencePortfolio/credit-card-risk-analyzer/:0:0","tags":["Sci-kit Learn","Logistic Regression","GridSearchCV"],"title":"Credit Card Risk Analyzer","uri":"/DataSciencePortfolio/credit-card-risk-analyzer/"},{"categories":["Machine Learning"],"content":"1. Exploratory Data Analysis I looked through the dataset to understand the various kinds of features present and their data types and statistics. There were columsn with missing and non-numeric type data. For the logistic regression model to perform well, the dataset needed some cleaning and pre-processing. ","date":"2019-10-01","objectID":"/DataSciencePortfolio/credit-card-risk-analyzer/:1:0","tags":["Sci-kit Learn","Logistic Regression","GridSearchCV"],"title":"Credit Card Risk Analyzer","uri":"/DataSciencePortfolio/credit-card-risk-analyzer/"},{"categories":["Machine Learning"],"content":"2. Data Cleaning There were columns with ‘?’ in place of correct data. To address this, I replaced the missing values with ‘NaN’. Next, the ‘NaN’s had to be treated. There are various ways to deal with such values. I chose to impute these values with the mean of the column. Lastly, to fill in the missing values in the columns with non-numeric data types, I replaced it with the most frequent observation in the column for that predictor. ","date":"2019-10-01","objectID":"/DataSciencePortfolio/credit-card-risk-analyzer/:2:0","tags":["Sci-kit Learn","Logistic Regression","GridSearchCV"],"title":"Credit Card Risk Analyzer","uri":"/DataSciencePortfolio/credit-card-risk-analyzer/"},{"categories":["Machine Learning"],"content":"3. Data Pre-Processing Firstly, the non-numeric data was encoded using LabelEncoding. Next, the data was scaled to bring it down to the [0-1] range. Normalizing the data improves model performance and it is computationally less expensive and since the data points are in the same range, each feature is weighted the same. Finally, the data was split into the training and testing dataset. ","date":"2019-10-01","objectID":"/DataSciencePortfolio/credit-card-risk-analyzer/:3:0","tags":["Sci-kit Learn","Logistic Regression","GridSearchCV"],"title":"Credit Card Risk Analyzer","uri":"/DataSciencePortfolio/credit-card-risk-analyzer/"},{"categories":["Machine Learning"],"content":"Code from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler(feature_range=(0, 1)) rescaledX_train = scaler.fit(X_train).transform(X_train) rescaledX_test = scaler.fit(X_test).transform(X_test) ","date":"2019-10-01","objectID":"/DataSciencePortfolio/credit-card-risk-analyzer/:3:1","tags":["Sci-kit Learn","Logistic Regression","GridSearchCV"],"title":"Credit Card Risk Analyzer","uri":"/DataSciencePortfolio/credit-card-risk-analyzer/"},{"categories":["Machine Learning"],"content":"4. Training and Testing Since, this is a binary classification problem, I chose to use the Logistic Regression algorithm for the prediction. The accuracy of the vanilla logistic regression model (without any tuning) was 83.7% ","date":"2019-10-01","objectID":"/DataSciencePortfolio/credit-card-risk-analyzer/:4:0","tags":["Sci-kit Learn","Logistic Regression","GridSearchCV"],"title":"Credit Card Risk Analyzer","uri":"/DataSciencePortfolio/credit-card-risk-analyzer/"},{"categories":["Machine Learning"],"content":"Code from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score y_pred = logreg.predict(rescaledX_test) print(\"Accuracy of logistic regression classifier: \", accuracy_score(y_test, y_pred)) confusion_matrix(y_test, y_pred) Accuracy of logistic regression classifier: 0.8377192982456141 array([[93, 10], [27, 98]]) ","date":"2019-10-01","objectID":"/DataSciencePortfolio/credit-card-risk-analyzer/:4:1","tags":["Sci-kit Learn","Logistic Regression","GridSearchCV"],"title":"Credit Card Risk Analyzer","uri":"/DataSciencePortfolio/credit-card-risk-analyzer/"},{"categories":["Machine Learning"],"content":"5. Hyperparameter Tuning To improve the accuracy of the model, I used GridSearchCV with cross-validation to find the optimum parameters for the model. Cross-validation works by splitting the data into k folds and fitting the data to the model. (k-1) sets are used for training and the last set is used for validation. ","date":"2019-10-01","objectID":"/DataSciencePortfolio/credit-card-risk-analyzer/:5:0","tags":["Sci-kit Learn","Logistic Regression","GridSearchCV"],"title":"Credit Card Risk Analyzer","uri":"/DataSciencePortfolio/credit-card-risk-analyzer/"},{"categories":["Machine Learning"],"content":"Code grid_model = GridSearchCV(logreg, param_grid=param_grid, cv=5) rescaledX = scaler.fit(X_train).transform(X_train) grid_model_result = grid_model.fit(rescaledX, y_train) # Summarize results best_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_ print(\"Best: %f using %s\" % (best_score, best_params)) Best: 0.863651 using {'max_iter': 100, 'tol': 0.01} ","date":"2019-10-01","objectID":"/DataSciencePortfolio/credit-card-risk-analyzer/:5:1","tags":["Sci-kit Learn","Logistic Regression","GridSearchCV"],"title":"Credit Card Risk Analyzer","uri":"/DataSciencePortfolio/credit-card-risk-analyzer/"},{"categories":["Machine Learning"],"content":"6. Results With an accuracy of 86.3%, this model can be used as a tool to identify customers who could default on their Credit Card bills. ","date":"2019-10-01","objectID":"/DataSciencePortfolio/credit-card-risk-analyzer/:6:0","tags":["Sci-kit Learn","Logistic Regression","GridSearchCV"],"title":"Credit Card Risk Analyzer","uri":"/DataSciencePortfolio/credit-card-risk-analyzer/"},{"categories":["Machine Learning"],"content":"Fun little AI bot that plays TicTacToe. In this fun project, the goal was to apply my understanding of Machine Learning to build a TicTacToe player. The learning problem was defined based on Tom Mitchell’s example learning problem of a checkers learning algorithm in his textbook. Setup Task T: playing TicTacToe Performance Measure P: Percentage of games won against humans Experience E: Indirect feedback via solution trace generated from games played against itself ","date":"2019-09-12","objectID":"/DataSciencePortfolio/tictactoe-ai/:0:0","tags":["Game","Python","LMS Rule"],"title":"TicTacToe AI","uri":"/DataSciencePortfolio/tictactoe-ai/"},{"categories":["Machine Learning"],"content":"Learning from Experience The target function (V) was chosen to be a linear function that maps a given board state to a real value (score). We then use an approximation algorithm Least Mean Squares to learn the target function from the solution trace. Learning V(board_state) = R, (score for a given board state) V_hat(board_state) = (w.T)*X (product of weights and corresponding feature values) The score (R) for each non-final board state isi assigned with the estimated score of the successor board state: V(board_state) = V_hat(successor(board_state)) V(final_board_state) = 100 (win) 0 (draw) -100 (loss) ","date":"2019-09-12","objectID":"/DataSciencePortfolio/tictactoe-ai/:0:1","tags":["Game","Python","LMS Rule"],"title":"TicTacToe AI","uri":"/DataSciencePortfolio/tictactoe-ai/"},{"categories":["Machine Learning"],"content":"Sample Game print(\"Let the game begin : \") play(size) Let the game begin : Computer's turn : -------------------- | - || - || - || X | -------------------- | - || - || - || - | -------------------- | - || - || - || - | -------------------- | - || - || - || - | -------------------- Human's move : Enter x coordinate : 0 Enter y coordinate : 0 -------------------- | O || - || - || X | -------------------- | - || - || - || - | -------------------- | - || - || - || - | -------------------- | - || - || - || - | -------------------- Computer's turn : -------------------- | O || - || - || X | -------------------- | X || - || - || - | -------------------- | - || - || - || - | -------------------- | - || - || - || - | -------------------- Human's move : Enter x coordinate : 3 Enter y coordinate : 1 -------------------- | O || - || - || X | -------------------- | X || - || - || - | -------------------- | - || - || - || - | -------------------- | - || O || - || - | -------------------- Computer's turn : -------------------- | O || - || - || X | -------------------- | X || X || - || - | -------------------- | - || - || - || - | -------------------- | - || O || - || - | -------------------- Human's move : Enter x coordinate : 3 Enter y coordinate : 2 -------------------- | O || - || - || X | -------------------- | X || X || - || - | -------------------- | - || - || - || - | -------------------- | - || O || O || - | -------------------- Computer's turn : -------------------- | O || - || - || X | -------------------- | X || X || X || - | -------------------- | - || - || - || - | -------------------- | - || O || O || - | -------------------- Human's move : Enter x coordinate : 3 Enter y coordinate : 3 -------------------- | O || - || - || X | -------------------- | X || X || X || - | -------------------- | - || - || - || - | -------------------- | - || O || O || O | -------------------- Computer's turn : -------------------- | O || - || - || X | -------------------- | X || X || X || - | -------------------- | - || - || - || - | -------------------- | X || O || O || O | -------------------- Human's move : Enter x coordinate : 2 Enter y coordinate : 1 -------------------- | O || - || - || X | -------------------- | X || X || X || - | -------------------- | - || O || - || - | -------------------- | X || O || O || O | -------------------- Computer's turn : -------------------- | O || - || - || X | -------------------- | X || X || X || X | -------------------- | - || O || - || - | -------------------- | X || O || O || O | -------------------- Computer Wins! ","date":"2019-09-12","objectID":"/DataSciencePortfolio/tictactoe-ai/:0:2","tags":["Game","Python","LMS Rule"],"title":"TicTacToe AI","uri":"/DataSciencePortfolio/tictactoe-ai/"},{"categories":["Machine Learning"],"content":"Building a handwritten digits and characters classifier from scratch. The goal of this project was to understand the inner workings of a Neural Network by coding the initialization, learning and testing algorithms from scratch using the NumPy library. ","date":"2019-08-01","objectID":"/DataSciencePortfolio/mnist-emnist/:0:0","tags":["MNIST","EMNIST","Python"],"title":"Handwritten Digits and Characters Classification","uri":"/DataSciencePortfolio/mnist-emnist/"},{"categories":["Machine Learning"],"content":"1. The Dataset MNIST: This is the well known handwritten digits dataset that is the “Hello World” of Computer Vision. It consists of images of digits (0-9) written by hand, each of size 28x28 pixels. MNIST: This is the handwritten characters dataset that contains images of both lower and upper case characters. Similar to the MNIST dataset, each image is of size 28x28 pixels. The images in these datasets were visualized using matplotlib. ","date":"2019-08-01","objectID":"/DataSciencePortfolio/mnist-emnist/:1:0","tags":["MNIST","EMNIST","Python"],"title":"Handwritten Digits and Characters Classification","uri":"/DataSciencePortfolio/mnist-emnist/"},{"categories":["Machine Learning"],"content":"Code sprint(\"Sample of images from the MNIST Dataset : \") plt.figure(figsize=(8, 8)) for i in range(16): plt.subplot(4, 4, i+1) plt.axis('off') r = np.random.randint(x_train.shape[0]) ## PICK A RANDON IMAGE TO SHOW plt.title('True Label: '+ str(y_train[r])) ## PRINT LABEL plt.imshow(x_train[r].reshape(28, 28)) ## PRINT IMAGE plt.show() ","date":"2019-08-01","objectID":"/DataSciencePortfolio/mnist-emnist/:1:1","tags":["MNIST","EMNIST","Python"],"title":"Handwritten Digits and Characters Classification","uri":"/DataSciencePortfolio/mnist-emnist/"},{"categories":["Machine Learning"],"content":"2. The N-layer Feed Forward Neural Network ","date":"2019-08-01","objectID":"/DataSciencePortfolio/mnist-emnist/:2:0","tags":["MNIST","EMNIST","Python"],"title":"Handwritten Digits and Characters Classification","uri":"/DataSciencePortfolio/mnist-emnist/"},{"categories":["Machine Learning"],"content":"Initializing Parameters for all layers # dim( W[l] ) = ( n[l], n[l-1] ) # dim( b[l] ) = ( 1, n[l] ) # where, # l = current layer # W[l] = weights of current layer # b[l] = bias for the current layer # n[l] = number of nodes in current layer def initialize_parameters(layer_dims): parameters = {} L = len(layer_dims) for i in range(1, L): parameters['W' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1])*0.01 parameters['b' + str(i)] = np.zeros((1, layer_dims[i])) + 0.01 return parameters ","date":"2019-08-01","objectID":"/DataSciencePortfolio/mnist-emnist/:2:1","tags":["MNIST","EMNIST","Python"],"title":"Handwritten Digits and Characters Classification","uri":"/DataSciencePortfolio/mnist-emnist/"},{"categories":["Machine Learning"],"content":"Forward and Backward Propagation # Forward propagation equations: # Z[l] = W[l].X + b[l] # A[l] = g( Z[l] ) # Where, # Z = weighted sum of input and bias # A = activations of particular layer # l = layer # Backward propagation equations: # Err(j)(output layer) = O(j)(1 - O(j))(T(j) - O(j)) # Err(j)(hidden layer) = O(j)(1 - O(j))(SUM(Err(k)W(j,k) # del(W(i,j)) = (l)Err(j)O(i) # del(b(j)) = (l)Err(j) # Where, # O: Output of a node # W: weight # b: bias # i, j, k: nodes def sigmoid(X): return 1/(1 + np.exp(-1*X)) def forward_step(A_prev, W, b): return sigmoid(np.dot(A_prev, W.T) + b) def computation_n(X, y, parameters, eta, num_iters): hidden_output = [] hidden_error = [] m = X.shape[0] L = (len(parameters)//2) + 1 # number of layers # iterating for given number of iterations for itr in range(num_iters): # for each training example for i in range(m): # forward propagation for n layers hidden_output.append(X[i]) A_prev = X[i] for l in range(1, L): A_prev = forward_step(A_prev, parameters['W' + str(l)], parameters['b' + str(l)]) hidden_output.append(A_prev) # propagating the error backwards # print('hidden_output[-1].shape: {}; y[i].shape: {}'.format(hidden_output[-1].shape, y[i].shape)) dOutput = hidden_output[-1]*(1 - hidden_output[-1])*(y[i] - hidden_output[-1]) hidden_error.append(dOutput) k = 0 for l in reversed(range(1, L-1)): error = hidden_output[l]*(1 - hidden_output[l])*np.dot(hidden_error[k], parameters['W' + str(l+1)]) hidden_error.append(error) k += 1 # parameter changes k = 0 for l in reversed(range(1, L)): parameters['W' + str(l)] += eta*hidden_error[k].reshape(-1, 1)*hidden_output[l-1] parameters['b' + str(l)] += eta*hidden_error[k] k += 1 hidden_output.clear() hidden_error.clear() return parameters ","date":"2019-08-01","objectID":"/DataSciencePortfolio/mnist-emnist/:2:2","tags":["MNIST","EMNIST","Python"],"title":"Handwritten Digits and Characters Classification","uri":"/DataSciencePortfolio/mnist-emnist/"},{"categories":["Machine Learning"],"content":"3. Training and Testing The MNIST and EMNIST datasets were loaded and the labels were One-Hot encoded. The parameters for the models were initialzed and the model was trained on the training split obtained by using the train_test_split method imported from sklearn.model_selection. Furthermore, the pixel values were normalized to the range [0-1] for more efficient computation. The results are shown below: ","date":"2019-08-01","objectID":"/DataSciencePortfolio/mnist-emnist/:3:0","tags":["MNIST","EMNIST","Python"],"title":"Handwritten Digits and Characters Classification","uri":"/DataSciencePortfolio/mnist-emnist/"},{"categories":["Machine Learning"],"content":"Code # Handwritten digits classification parameters = initialize_parameters([784, 50, 50, 10]) print(\"Length of parameters dictionary : {}\".format(len(parameters))) Length of parameters dictionary : 6 print(\"Training model...\") parameters = train(x_train, y_train, parameters, 0.01, 20) Training model... print(\"Testing model...\") print('Training error : ', end='') test(x_train, y_train, parameters) print('\\nTesting error : ', end='') test(x_test, y_test, parameters) Testing model... Training error : Accuracy : 98.791 % Testing error : Accuracy : 98.772 % # Handwritten characters classification parameters = initialize_parameters([784, 50, 50, 27]) print(\"Length of parameters dictionary : {}\".format(len(parameters))) Length of parameters dictionary : 6 print(\"Training model...\") parameters = train(X_train, y_train, parameters, 0.01, 20) Training model... print(\"Testing model...\") test(X_test, y_test, parameters) Testing model... Accuracy : 96.29629629629629 % ","date":"2019-08-01","objectID":"/DataSciencePortfolio/mnist-emnist/:3:1","tags":["MNIST","EMNIST","Python"],"title":"Handwritten Digits and Characters Classification","uri":"/DataSciencePortfolio/mnist-emnist/"},{"categories":["Machine Learning"],"content":"4. Results By coding a Neural Network from scratch, I was able to get a better understanding of the math behind the magic. ","date":"2019-08-01","objectID":"/DataSciencePortfolio/mnist-emnist/:4:0","tags":["MNIST","EMNIST","Python"],"title":"Handwritten Digits and Characters Classification","uri":"/DataSciencePortfolio/mnist-emnist/"},{"categories":null,"content":"About","date":"0001-01-01","objectID":"/DataSciencePortfolio/about/","tags":null,"title":"About","uri":"/DataSciencePortfolio/about/"},{"categories":null,"content":"Hi there! ","date":"0001-01-01","objectID":"/DataSciencePortfolio/about/:0:0","tags":null,"title":"About","uri":"/DataSciencePortfolio/about/"},{"categories":null,"content":"My passion I attribute my passion for computers and code to a couple of events in my life: First was when I stumbled upon a YouTube tutorial on building web pages using HTML in the eighth grade. The fact that I could have a website of my own was exhilarating. Next was when I learned to make the computer “think” and differentiate between images of cats and dogs. ","date":"0001-01-01","objectID":"/DataSciencePortfolio/about/:0:1","tags":null,"title":"About","uri":"/DataSciencePortfolio/about/"},{"categories":null,"content":"What I’m doing now Programs that identify cats are inane yet showcase the breadth of applications machine learning has. In my opinion, Artificial Intelligence will become one of the cornerstones of technologies in the future. As a student specializing in Data Science at Carnegie Mellon University, I am excited to be a part of this new paradigm. Courses like Deep Learning, Data Science and Big Data, and Machine Learning will equip me with the tools required to be a successful data scientist. ","date":"0001-01-01","objectID":"/DataSciencePortfolio/about/:0:2","tags":null,"title":"About","uri":"/DataSciencePortfolio/about/"},{"categories":null,"content":"What keeps me going Curiosity is what makes me tick. I can spend hours consuming content from all corners of the internet. This helps me develop a firm understanding of concepts after which, I look for ways to incorporate those ideas into my work. In the past, I realigned content on web pages in a more visually pleasing manner, rewrote code for efficiency, and fine-tuned documentation. I am always open to feedback and look to improve as I go along. ","date":"0001-01-01","objectID":"/DataSciencePortfolio/about/:0:3","tags":null,"title":"About","uri":"/DataSciencePortfolio/about/"},{"categories":null,"content":"What I do when I’m not working During my downtime, you will find me discovering new music, playing pool, analyzing stocks, and cracking open a cold one with friends. Whenever possible, I watch Formula 1 races and episodes of The Office. ","date":"0001-01-01","objectID":"/DataSciencePortfolio/about/:0:4","tags":null,"title":"About","uri":"/DataSciencePortfolio/about/"},{"categories":null,"content":"Resume ","date":"0001-01-01","objectID":"/DataSciencePortfolio/about/:0:5","tags":null,"title":"About","uri":"/DataSciencePortfolio/about/"}]